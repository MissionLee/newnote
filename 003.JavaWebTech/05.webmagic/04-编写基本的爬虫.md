# 编写
在WebMagic里，实现一个基本的爬虫只需要编写一个类，实现PageProcessor接口即可。这个类基本上包含了抓取一个网站，你需要写的所有代码。

同时这部分还会介绍如何使用WebMagic的抽取API，以及最常见的抓取结果保存的问题。

## 4.1 实现PageProcessor
这部分我们直接通过GithubRepoPageProcessor这个例子来介绍PageProcessor的编写方式。我将PageProcessor的定制分为三个部分，分别是爬虫的配置、页面元素的抽取和链接的发现。
```java
public class GithubRepoPageProcessor implements PageProcessor {

    // 部分一：抓取网站的相关配置，包括编码、抓取间隔、重试次数等
    private Site site = Site.me().setRetryTimes(3).setSleepTime(1000);

    @Override
    // process是定制爬虫逻辑的核心接口，在这里编写抽取逻辑
    public void process(Page page) {
        // 部分二：定义如何抽取页面信息，并保存下来
        page.putField("author", page.getUrl().regex("https://github\\.com/(\\w+)/.*").toString());
        page.putField("name", page.getHtml().xpath("//h1[@class='entry-title public']/strong/a/text()").toString());
        if (page.getResultItems().get("name") == null) {
            //skip this page
            page.setSkip(true);
        }
        page.putField("readme", page.getHtml().xpath("//div[@id='readme']/tidyText()"));

        // 部分三：从页面发现后续的url地址来抓取
        page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/[\\w\\-]+/[\\w\\-]+)").all());
    }

    @Override
    public Site getSite() {
        return site;
    }

    public static void main(String[] args) {

        Spider.create(new GithubRepoPageProcessor())
                //从"https://github.com/code4craft"开始抓
                .addUrl("https://github.com/code4craft")
                //开启5个线程抓取
                .thread(5)
                //启动爬虫
                .run();
    }
}
```
### 爬虫的配置
第一部分关于爬虫的配置，包括编码、抓取间隔、超时时间、重试次数等，也包括一些模拟的参数，例如User Agent、cookie，以及代理的设置，我们会在第5章-“爬虫的配置”里进行介绍。在这里我们先简单设置一下：重试次数为3次，抓取间隔为一秒。

### 页面元素的抽取

第二部分是爬虫的核心部分：对于下载到的Html页面，你如何从中抽取到你想要的信息？WebMagic里主要使用了三种抽取技术：XPath、正则表达式和CSS选择器。另外，对于JSON格式的内容，可使用JsonPath进行解析。

> XPath

XPath本来是用于XML中获取元素的一种查询语言，但是用于Html也是比较方便的。例如：
```java
 page.getHtml().xpath("//h1[@class='entry-title public']/strong/a/text()")
```
这段代码使用了XPath，它的意思是“查找所有class属性为'entry-title public'的h1元素，并找到他的strong子节点的a子节点，并提取a节点的文本信息”。 对应的Html是这样子的：



> CSS选择器

CSS选择器是与XPath类似的语言。如果大家做过前端开发，肯定知道$('h1.entry-title')这种写法的含义。客观的说，它比XPath写起来要简单一些，但是如果写复杂一点的抽取规则，就相对要麻烦一点。

> 正则表达式

正则表达式则是一种通用的文本抽取语言。
```java
 page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
```
这段代码就用到了正则表达式，它表示匹配所有`"https://github.com/code4craft/webmagic"`这样的链接。

> JsonPath

JsonPath是于XPath很类似的一个语言，它用于从Json中快速定位一条内容。WebMagic中使用的JsonPath格式可以参考这里：https://code.google.com/p/json-path/

### 链接的发现

有了处理页面的逻辑，我们的爬虫就接近完工了！

但是现在还有一个问题：一个站点的页面是很多的，一开始我们不可能全部列举出来，于是如何发现后续的链接，是一个爬虫不可缺少的一部分。
```java
page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
```
这段代码的分为两部分
  - `page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all()`用于获取所有满足"(https:/ /github\.com/\w+/\w+)"这个正则表达式的链接
  - `page.addTargetRequests()`则将这些链接加入到待抓取的队列中去。

## 使用Selectable抽取元素

Selectable相关的抽取元素链式API是WebMagic的一个核心功能。使用Selectable接口，你可以直接完成页面元素的链式抽取，也无需去关心抽取的细节。

在刚才的例子中可以看到，`page.getHtml()`返回的是一个Html对象，它实现了Selectable接口。这个接口包含一些重要的方法，我将它分为两类：抽取部分和获取结果部分。

### 抽取部分API：
|方法	|说明	|示例
|-|-|-
|xpath(String xpath)	|使用XPath选择	|html.xpath("//div[@class='title']")
|$(String selector)	|使用Css选择器选择	|html.$("div.title")
|$(String selector,String attr)	|使用Css选择器选择	|html.$("div.title","text")
|css(String selector)	|功能同$()，使用Css选择器选择	|html.css("div.title")
|links()	|选择所有链接	|html.links()
|regex(String regex)	|使用正则表达式抽取	|html.regex("\(.\*?)\")
|regex(String regex,int group)	|使用正则表达式抽取，并指定捕获组	|html.regex("\(.\*?)\",1)
|replace(String regex, String replacement)	|替换内容	|html.replace("\","")

这部分抽取API返回的都是一个`Selectable`接口，意思是说，抽取是支持链式调用的。下面我用一个实例来讲解链式API的使用。

例如，我现在要抓取github上所有的Java项目，这些项目可以在https://github.com/search?l=Java&p=1&q=stars%3A%3E1&s=stars&type=Repositories 搜索结果中看到。

为了避免抓取范围太宽，我指定只从分页部分抓取链接。这个抓取规则是比较复杂的，我会要怎么写呢？


那么我可以先用CSS选择器提取出这个div，然后在取到所有的链接。为了保险起见，我再使用正则表达式限定一下提取出的URL的格式，那么最终的写法是这样子的：
```java
List<String> urls = page.getHtml()
    .css("div.pagination") //限定找到目标 div
    .links() // 获取其中 link
    .regex(".*/search/\?l=java.*") // 限定url格式
    .all();
```
然后，我们可以把这些URL加到抓取列表中去：
```java
List<String> urls = page.getHtml().css("div.pagination").links().regex(".*/search/\?l=java.*").all();
page.addTargetRequests(urls);
```
是不是比较简单？除了发现链接，Selectable的链式抽取还可以完成很多工作。我们会在第9章示例中再讲到。

### 获取结果的API：

当链式调用结束时，我们一般都想要拿到一个字符串类型的结果。这时候就需要用到获取结果的API了。我们知道，一条抽取规则，无论是XPath、CSS选择器或者正则表达式，总有可能抽取到多条元素。WebMagic对这些进行了统一，你可以通过不同的API获取到一个或者多个元素。

|方法	|说明	|示例
|-|-|-
|get()	|返回一条String类型的结果	|String link= html.links().get()
|toString()	|功能同get()，返回一条String类型的结果	|String link= html.links().toString()
|all()	|返回所有抽取结果	|List links= html.links().all()
|match()	|是否有匹配结果	|if (html.links().match()){ xxx; }

例如，我们知道页面只会有一条结果，那么可以使用selectable.get()或者selectable.toString()拿到这条结果。

这里selectable.toString()采用了toString()这个接口，是为了在输出以及和一些框架结合的时候，更加方便。因为一般情况下，我们都只需要选择一个元素！

selectable.all()则会获取到所有元素。

好了，到现在为止，在回过头看看3.1中的GithubRepoPageProcessor，可能就觉得更加清晰了吧？指定main方法，已经可以看到抓取结果在控制台输出了。

## 使用Pipeline保存结果
好了，爬虫编写完成，现在我们可能还有一个问题：我如果想把抓取的结果保存下来，要怎么做呢？WebMagic用于保存结果的组件叫做Pipeline。例如我们通过“控制台输出结果”这件事也是通过一个内置的Pipeline完成的，它叫做ConsolePipeline。那么，我现在想要把结果用Json的格式保存下来，怎么做呢？我只需要将Pipeline的实现换成"JsonFilePipeline"就可以了。
```java
public static void main(String[] args) {
    Spider.create(new GithubRepoPageProcessor())
            //从"https://github.com/code4craft"开始抓
            .addUrl("https://github.com/code4craft")
            .addPipeline(new JsonFilePipeline("D:\\webmagic\\"))
            //开启5个线程抓取
            .thread(5)
            //启动爬虫
            .run();
}
```

这样子下载下来的文件就会保存在D盘的webmagic目录中了。

通过定制Pipeline，我们还可以实现保存结果到文件、数据库等一系列功能。这个会在第7章“抽取结果的处理”中介绍。

至此为止，我们已经完成了一个基本爬虫的编写，也具有了一些定制功能。

## 爬虫的配置、启动和终止
### Spider

Spider是爬虫启动的入口。在启动爬虫之前，我们需要使用一个PageProcessor创建一个Spider对象，然后使用run()进行启动。同时Spider的其他组件（Downloader、Scheduler、Pipeline）都可以通过set方法来进行设置。

|方法	|说明	|示例
|-|-|-
|create(PageProcessor)	|创建Spider	|Spider.create(new GithubRepoProcessor())
|addUrl(String…)	|添加初始的URL	|spider .addUrl("http://webmagic.io/docs/")
|addRequest(Request...)	|添加初始的Request	|spider .addRequest("http://webmagic.io/docs/")
|thread(n)	|开启n个线程	|spider.thread(5)
|run()	|启动，会阻塞当前线程执行	|spider.run()
|start()/runAsync()	|异步启动，当前线程继续执行	|spider.start()
|stop()	|停止爬虫	|spider.stop()
|test(String)	|抓取一个页面进行测试	|spider .test("http://webmagic.io/docs/")
|addPipeline(Pipeline)	|添加一个Pipeline，一个Spider可以有多个Pipeline	|spider .addPipeline(new ConsolePipeline())
|setScheduler(Scheduler)	|设置Scheduler，一个Spider只能有个一个Scheduler	|spider.setScheduler(new RedisScheduler())
|setDownloader(Downloader)	|设置Downloader，一个Spider只能有个一个Downloader	|spider .setDownloader(new SeleniumDownloader())
|get(String)	|同步调用，并直接取得结果	|ResultItems result = spider .get("http://webmagic.io/docs/")
|getAll(String…)	|同步调用，并直接取得一堆结果	|List<ResultItems> results = spider .getAll("http://webmagic.io/docs/", "http://webmagic.io/xxx")

## Site

对站点本身的一些配置信息，例如编码、HTTP头、超时时间、重试策略等、代理等，都可以通过设置Site对象来进行配置。

|方法	|说明	|示例
|-|-|-
|setCharset(String)	|设置编码	|site.setCharset("utf-8")
|setUserAgent(String)	|设置UserAgent	|site.setUserAgent("Spider")
|setTimeOut(int)	|设置超时时间，单位是毫秒	|site.setTimeOut(3000)
|setRetryTimes(int)	|设置重试次数	|site.setRetryTimes(3)
|setCycleRetryTimes(int)	|设置循环重试次数	|site.setCycleRetryTimes(3)
|addCookie(String,String)	|添加一条cookie	|site.addCookie("dotcomt_user","code4craft")
|setDomain(String)	|设置域名，需设置域名后，addCookie才可生效	|site.setDomain("github.com")
|addHeader(String,String)	|添加一条addHeader	|site.addHeader("Referer","https://github.com")
|setHttpProxy(HttpHost)	|设置Http代理	|site.setHttpProxy(new HttpHost("127.0.0.1",8080))

其中循环重试cycleRetry是0.3.0版本加入的机制。

该机制会将下载失败的url重新放入队列尾部重试，直到达到重试次数，以保证不因为某些网络原因漏抓页面。

## Jsoup和Xsoup

WebMagic的抽取主要用到了Jsoup和我（原作者）自己开发的工具Xsoup。

### Jsoup
Jsoup是一个简单的HTML解析器，同时它支持使用CSS选择器的方式查找元素。为了开发WebMagic，我对Jsoup的源码进行过详细的分析，具体文章参见Jsoup学习笔记。

### Xsoup
Xsoup是我基于Jsoup开发的一款XPath解析器。

之前WebMagic使用的解析器是`HtmlCleaner`，使用过程存在一些问题。主要问题是XPath出错定位不准确，并且其不太合理的代码结构，也难以进行定制。最终我自己实现了Xsoup，使得更加符合爬虫开发的需要。令人欣喜的是，经过测试，Xsoup的性能比HtmlCleaner要快一倍以上。

Xsoup发展到现在，已经支持爬虫常用的语法，以下是一些已支持的语法对照表：

|Name	|Expression	|Support
|-|-|-
|nodename	|nodename	|yes
|immediate parent	|/	|yes
|parent	|//	yes
|attribute	|[@key=value]	|yes
|nth child	|tag[n]	|yes
|attribute	|/@key	|yes
|wildcard in tagname	|/*	|yes
|wildcard in attribute	|/[@*]	|yes
|function	|function()	|part
|or	|a | b	|yes since 0.2.0
|parent in path	|. or ..	|no
|predicates	|price>35	|no
|predicates logic	|@class=a or @class=b	|yes since 0.2.0

另外我自己定义了几个对于爬虫来说，很方便的XPath函数。但是请注意，这些函数式标准XPath没有的。

|Expression	|Description	|XPath1.0
|-|-|-
|text(n)	|第n个直接文本子节点，为0表示所有	|text() only
|allText()	|所有的直接和间接文本子节点	|not support
|tidyText()	|所有的直接和间接文本子节点，并将一些标签替换为换行，使纯文本显示更整洁	|not support
|html()	|内部html，不包括标签的html本身	|not support
|outerHtml()	|内部html，包括标签的html本身	|not support
|regex(@attr,expr,group)	|这里@attr和group均可选，默认是group0	|not support

### Saxon
Saxon是一个强大的XPath解析器，支持XPath 2.0语法。webmagic-saxon是对Saxon尝试性的一个整合，但是目前看来，XPath 2.0的高级语法，似乎在爬虫开发中使用者并不多。

## 爬虫的监控 [此部分参考存下来的网页内容]

爬虫的监控是0.5.0新增的功能。利用这个功能，你可以查看爬虫的执行情况——已经下载了多少页面、还有多少页面、启动了多少线程等信息。该功能通过JMX实现，你可以使用Jconsole等JMX工具查看本地或者远程的爬虫信息。

如果你完全不会JMX也没关系，因为它的使用相对简单，本章会比较详细的讲解使用方法。如果要弄明白其中原理，你可能需要一些JMX的知识，推荐阅读：JMX整理。我很多部分也对这篇文章进行了参考。

注意: 如果你自己定义了Scheduler，那么需要用这个类实现MonitorableScheduler接口，才能查看“LeftPageCount”和“TotalPageCount”这两条信息。

### 为项目添加监控
添加监控非常简单，获取一个SpiderMonitor的单例SpiderMonitor.instance()，并将你想要监控的Spider注册进去即可。你可以注册多个Spider到SpiderMonitor中。
```java
public class MonitorExample {

    public static void main(String[] args) throws Exception {

        Spider oschinaSpider = Spider.create(new OschinaBlogPageProcessor())
                .addUrl("http://my.oschina.net/flashsword/blog");
        Spider githubSpider = Spider.create(new GithubRepoPageProcessor())
                .addUrl("https://github.com/code4craft");

        SpiderMonitor.instance().register(oschinaSpider);
        SpiderMonitor.instance().register(githubSpider);
        oschinaSpider.start();
        githubSpider.start();
    }
}
```
### 查看监控信息
WebMagic的监控使用JMX提供控制，你可以使用任何支持JMX的客户端来进行连接。我们这里以JDK自带的JConsole为例。我们首先启动WebMagic的一个Spider，并添加监控代码。然后我们通过JConsole来进行查看。

我们按照4.6.1的例子启动程序，然后在命令行输入jconsole（windows下是在DOS下输入jconsole.exe）即可启动JConsole。

jconsole

这里我们选择启动WebMagic的本地进程，连接后选择“MBean”，点开“WebMagic”，就能看到所有已经监控的Spider信息了！

这里我们也可以选择“操作”，在操作里可以选择启动-start()和终止爬虫-stop()，这会直接调用对应Spider的start()和stop()方法，来达到基本控制的目的。

jconsole-show

### 扩展监控接口
除了已有的一些监控信息，如果你有更多的信息需要监控，也可以通过扩展的方式来解决。你可以通过继承SpiderStatusMXBean来实现扩展，具体例子可以看这里： 定制扩展demo。

## 配置代理
从0.7.1版本开始，WebMagic开始使用了新的代理APIProxyProvider。因为相对于Site的“配置”，ProxyProvider定位更多是一个“组件”，所以代理不再从Site设置，而是由HttpClientDownloader设置。

HttpClientDownloader.setProxyProvider(ProxyProvider proxyProvider)	设置代理


ProxyProvider有一个默认实现：`SimpleProxyProvider`。它是一个基于简单Round-Robin的、没有失败检查的ProxyProvider。可以配置任意个候选代理，每次会按顺序挑选一个代理使用。它适合用在自己搭建的比较稳定的代理的场景。

代理示例：

设置单一的普通HTTP代理为`101.101.101.101`的`8888`端口，并设置密码为"username","password"
```java
    HttpClientDownloader httpClientDownloader = new HttpClientDownloader();
    httpClientDownloader.setProxyProvider(SimpleProxyProvider.from(new Proxy("101.101.101.101",8888,"username","password")));
    spider.setDownloader(httpClientDownloader);
```
设置代理池，其中包括`101.101.101.101`和`102.102.102.102`两个IP，没有密码
```java
    HttpClientDownloader httpClientDownloader = new HttpClientDownloader();
    httpClientDownloader.setProxyProvider(SimpleProxyProvider.from(
    new Proxy("101.101.101.101",8888)
    ,new Proxy("102.102.102.102",8888)));
```
如果对于代理部分有建议的，欢迎参与讨论#579 更多的代理ProxyProvider实现

## 处理非HTTP GET请求
一般来说，爬虫只会抓取信息展示类的页面，所以基本只会处理HTTP GET方法的数据。但是对于某些场景，模拟POST等方法也是需要的。

0.7.1版本之后，废弃了老的nameValuePair的写法，采用在Request对象上添加Method和requestBody来实现。
```java
Request request = new Request("http://xxx/path");
request.setMethod(HttpConstant.Method.POST);
request.setRequestBody(HttpRequestBody.json("{'id':1}","utf-8"));
```
HttpRequestBody内置了几种初始化方式，支持最常见的表单提交、json提交等方式。

|API	|说明
|-|-
|HttpRequestBody.form(Map\ params, String encoding)	|使用表单提交的方式
|HttpRequestBody.json(String json, String encoding)	|使用JSON的方式，json是序列化后的结果
|HttpRequestBody.xml(String xml, String encoding)	|设置xml的方式，xml是序列化后的结果
|HttpRequestBody.custom(byte[] body, String contentType, String encoding)	|设置自定义的requestBody

POST的去重：

从0.7.1版本开始，POST默认不会去重，详情见：Issue 484。如果想要去重可以自己继承DuplicateRemovedScheduler，重写push方法。

## 使用注解编写爬虫

此部分等玩熟练了再来看看

## 组件的使用和定制
在第一章里，我们提到了WebMagic的组件。WebMagic的一大特色就是可以灵活的定制组件功能，实现你自己想要的功能。

在Spider类里，PageProcessor、Downloader、Scheduler和Pipeline四个组件都是Spider的字段。除了PageProcessor是在Spider创建的时候已经指定，Downloader、Scheduler和Pipeline都可以通过Spider的setter方法来进行配置和更改。

|方法	|说明	|示例
|setScheduler()	|设置Scheduler	|spipder.setScheduler(new FileCacheQueueScheduler("D:\data\webmagic"))
|setDownloader()	|设置Downloader	|spipder.setDownloader(new SeleniumDownloader()))
|addPipeline()	|设置Pipeline，一个Spider可以有多个Pipeline	|spipder.addPipeline(new FilePipeline())

在这一章，我们会讲到如何定制这些组件，完成我们想要的功能。

### 使用和定制Pipeline
Pileline是抽取结束后，进行处理的部分，它主要用于抽取结果的保存，也可以定制Pileline可以实现一些通用的功能。在这一节中，我们会对Pipeline进行介绍，并用两个例子来讲解如何定制Pipeline。

> Pipeline介绍
Pipeline的接口定义如下：
```java
public interface Pipeline {

    // ResultItems保存了抽取结果，它是一个Map结构，
    // 在page.putField(key,value)中保存的数据，可以通过ResultItems.get(key)获取
    public void process(ResultItems resultItems, Task task);

}
```
可以看到，Pipeline其实就是将PageProcessor抽取的结果，继续进行了处理的，其实在Pipeline中完成的功能，你基本上也可以直接在PageProcessor实现，那么为什么会有Pipeline？有几个原因：

为了模块分离。“页面抽取”和“后处理、持久化”是爬虫的两个阶段，将其分离开来，一个是代码结构比较清晰，另一个是以后也可能将其处理过程分开，分开在独立的线程以至于不同的机器执行。
Pipeline的功能比较固定，更容易做成通用组件。每个页面的抽取方式千变万化，但是后续处理方式则比较固定，例如保存到文件、保存到数据库这种操作，这些对所有页面都是通用的。WebMagic中就已经提供了控制台输出、保存到文件、保存为JSON格式的文件几种通用的Pipeline。

在WebMagic里，一个Spider可以有多个Pipeline，使用Spider.addPipeline()即可增加一个Pipeline。这些Pipeline都会得到处理，例如你可以使用
```java
spider.addPipeline(new ConsolePipeline()).addPipeline(new FilePipeline())
```
实现输出结果到控制台，并且保存到文件的目标。

- 将结果输出到控制台

在介绍PageProcessor时，我们使用了GithubRepoPageProcessor作为例子，其中某一段代码中，我们将结果进行了保存：
```java
public void process(Page page) {
    page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
    page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+)").all());
    //保存结果author，这个结果会最终保存到ResultItems中
    page.putField("author", page.getUrl().regex("https://github\\.com/(\\w+)/.*").toString());
    page.putField("name", page.getHtml().xpath("//h1[@class='entry-title public']/strong/a/text()").toString());
    if (page.getResultItems().get("name")==null){
        //设置skip之后，这个页面的结果不会被Pipeline处理
        page.setSkip(true);
    }
    page.putField("readme", page.getHtml().xpath("//div[@id='readme']/tidyText()"));
}
```
现在我们想将结果保存到控制台，要怎么做呢？ConsolePipeline可以完成这个工作：
```java
public class ConsolePipeline implements Pipeline {

    @Override
    public void process(ResultItems resultItems, Task task) {
        System.out.println("get page: " + resultItems.getRequest().getUrl());
        //遍历所有结果，输出到控制台，上面例子中的"author"、"name"、"readme"都是一个key，其结果则是对应的value
        for (Map.Entry<String, Object> entry : resultItems.getAll().entrySet()) {
            System.out.println(entry.getKey() + ":\t" + entry.getValue());
        }
    }
}
```

参考这个例子，你就可以定制自己的Pipeline了——从ResultItems中取出数据，再按照你希望的方式处理即可。

> 将结果保存到MySQL
这里先介绍一个demo项目：jobhunter。它是一个集成了Spring，使用WebMagic抓取招聘信息，并且使用Mybatis持久化到Mysql的例子。我们会用这个项目来介绍如果持久化到Mysql。

在Java里，我们有很多方式将数据保存到MySQL，例如jdbc、dbutils、spring-jdbc、MyBatis等工具。这些工具都可以完成同样的事情，只不过功能和使用复杂程度不一样。如果使用jdbc，那么我们只需要从ResultItems取出数据，进行保存即可。

如果我们会使用ORM框架来完成持久化到MySQL的工作，就会面临一个问题：这些框架一般都要求保存的内容是一个定义好结构的对象，而不是一个key-value形式的ResultItems。以MyBatis为例，我们使用MyBatis-Spring可以定义这样一个DAO：
```java
public interface JobInfoDAO {

    @Insert("insert into JobInfo (`title`,`salary`,`company`,`description`,`requirement`,`source`,`url`,`urlMd5`) values (#{title},#{salary},#{company},#{description},#{requirement},#{source},#{url},#{urlMd5})")
    public int add(LieTouJobInfo jobInfo);
}
```
我们要做的，就是实现一个Pipeline，将ResultItems和LieTouJobInfo对象结合起来。

> 注解模式

注解模式下，WebMagic内置了一个PageModelPipeline：
```java
public interface PageModelPipeline<T> {

    //这里传入的是处理好的对象
    public void process(T t, Task task);

}
```
这时，我们可以很优雅的定义一个JobInfoDaoPipeline，来实现这个功能：
```java
@Component("JobInfoDaoPipeline")
public class JobInfoDaoPipeline implements PageModelPipeline<LieTouJobInfo> {

    @Resource
    private JobInfoDAO jobInfoDAO;

    @Override
    public void process(LieTouJobInfo lieTouJobInfo, Task task) {
        //调用MyBatis DAO保存结果
        jobInfoDAO.add(lieTouJobInfo);
    }
}
```
基本Pipeline模式

至此，结果保存就已经完成了！那么如果我们使用原始的Pipeline接口，要怎么完成呢？其实答案也很简单，如果你要保存一个对象，那么就需要在抽取的时候，将它保存为一个对象：
```java
public void process(Page page) {
    page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+/\\w+)").all());
    page.addTargetRequests(page.getHtml().links().regex("(https://github\\.com/\\w+)").all());
    GithubRepo githubRepo = new GithubRepo();
    githubRepo.setAuthor(page.getUrl().regex("https://github\\.com/(\\w+)/.*").toString());
    githubRepo.setName(page.getHtml().xpath("//h1[@class='entry-title public']/strong/a/text()").toString());
    githubRepo.setReadme(page.getHtml().xpath("//div[@id='readme']/tidyText()").toString());
    if (githubRepo.getName() == null) {
        //skip this page
        page.setSkip(true);
    } else {
        page.putField("repo", githubRepo);
    }
}
```
在Pipeline中，只要使用
```java
GithubRepo githubRepo = (GithubRepo)resultItems.get("repo");
```
就可以获取这个对象了。

PageModelPipeline实际上也是通过原始的Pipeline来实现的，它将与PageProcessor进行了整合，在保存时，使用类名作为key，而对象则是value，具体实现见：ModelPipeline。

> WebMagic已经提供的几个Pipeline
WebMagic中已经提供了将结果输出到控制台、保存到文件和JSON格式保存的几个Pipeline：

|类	|说明	|备注
|ConsolePipeline	|输出结果到控制台	|抽取结果需要实现toString方法
|FilePipeline	|保存结果到文件	抽取结果需要实现toString方法
|JsonFilePipeline	|JSON格式保存结果到文件	
|ConsolePageModelPipeline	|(注解模式)输出结果到控制台	
|FilePageModelPipeline	|(注解模式)保存结果到文件	
|JsonFilePageModelPipeline	|(注解模式)JSON格式保存结果到文件	|想要持久化的字段需要有getter方法

### 使用和定制Scheduler
Scheduler是WebMagic中进行URL管理的组件。一般来说，Scheduler包括两个作用：

对待抓取的URL队列进行管理。

对已抓取的URL进行去重。

WebMagic内置了几个常用的Scheduler。如果你只是在本地执行规模比较小的爬虫，那么基本无需定制Scheduler，但是了解一下已经提供的几个Scheduler还是有意义的。

|类	|说明	|备注
|DuplicateRemovedScheduler	|抽象基类，提供一些模板方法	|继承它可以实现自己的功能
|QueueScheduler	|使用内存队列保存待抓取URL	
|PriorityScheduler	|使用带有优先级的内存队列保存待抓取URL	|耗费内存较QueueScheduler更大，但是当设置了request.priority之后，只能使用PriorityScheduler才可使优先级生效
|FileCacheQueueScheduler	|使用文件保存抓取URL，可以在关闭程序并下次启动时，从之前抓取到的URL继续抓取	|需指定路径，会建立.urls.txt和.cursor.txt两个文件
|RedisScheduler|	使用Redis保存抓取队列，可进行多台机器同时合作抓取	|需要安装并启动redis

在0.5.1版本里，我对Scheduler的内部实现进行了重构，去重部分被单独抽象成了一个接口：DuplicateRemover，从而可以为同一个Scheduler选择不同的去重方式，以适应不同的需要，目前提供了两种去重方式。

|类	|说明
|HashSetDuplicateRemover	|使用HashSet来进行去重，占用内存较大
|BloomFilterDuplicateRemover	|使用BloomFilter来进行去重，占用内存较小，但是可能漏抓页面

所有默认的Scheduler都使用HashSetDuplicateRemover来进行去重，（除开RedisScheduler是使用Redis的set进行去重）。如果你的URL较多，使用HashSetDuplicateRemover会比较占用内存，所以也可以尝试以下BloomFilterDuplicateRemover1，使用方式：
```java
spider.setScheduler(new QueueScheduler()
.setDuplicateRemover(new BloomFilterDuplicateRemover(10000000)) //10000000是估计的页面数量
)
```

 0.6.0版本后，如果使用BloomFilterDuplicateRemover，需要单独引入Guava依赖包。 ↩

### 使用和定制Downloader
WebMagic的默认Downloader基于HttpClient。一般来说，你无须自己实现Downloader，不过HttpClientDownloader也预留了几个扩展点，以满足不同场景的需求。

另外，你可能希望通过其他方式来实现页面下载，例如使用SeleniumDownloader来渲染动态页面。

